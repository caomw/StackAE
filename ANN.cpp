/*******************************************************************
 *  Copyright(c) 2015
 *  All rights reserved.
 *
 *  Name: Artificial Neural Network
 *  Description: A multi perceptron to study the correlation between data
 *
 *  Date: 2015-9-4
 *  Author: Yang
 *  Intruction: Use 50000 handwritten digit images to train the MLP and
                test with 10000 images.  Data from MNIST.

 ******************************************************************/
#include "ANN.h"

NeuralNetwork::NeuralNetwork(int nLayers, UINT iCount, UINT oCount):
            nLayers(nLayers), inputVectorDim(iCount), outputVectorDim(oCount)
{
    /*
     *  Description:
     *  A constructed function to initialize the Neural Network
     *
     *  @param nLayers: total layers including input and output
     *  @param iCount: input vector dimension(e.g. MNIST 784)
     *  @param oCount: output vector dimension(e.g. MNIST 10)
     *
     */
    
    std::cout << "Please input the hidden units of every layers" << std::endl;
    
    /* Set the number of units in every layers */
    Neurons.push_back(iCount);
    for (int i = 0; i < nLayers - 2; i++)
    {
        int HiddenUnits;
        std::cin >> HiddenUnits;
        Neurons.push_back(HiddenUnits);
    }
    Neurons.push_back(oCount);
    
    /* Initialize the layer construction */
    NNLayer *Layer;
    Layer = new NNLayer(0, iCount);
    m_Layers.push_back(Layer);
    
    int i = 0;
    for (; i < nLayers - 2; i++)
    {
        /* The parameters in constructed function is to build weight matrix */
        Layer = new NNLayer(Neurons[i], Neurons[i + 1], Layer);
        m_Layers.push_back(Layer);
    }
    Layer = new NNLayer(Neurons[i], oCount, Layer);
    m_Layers.push_back(Layer);
}

NeuralNetwork::NeuralNetwork(int nLayers, std::vector<UINT> Units):
            nLayers(nLayers)
{
    /*
     *  Description:
     *  A constructed function to initialize the Neural Network with known
     *  weight
     *
     *  @param nLayers: total layers including input and output
     *  @param Units: vector containing units including i/o units
     *
     */
    
    assert(nLayers == Units.size() && nLayers > 1);

    inputVectorDim = *(Units.begin());
    outputVectorDim = *(Units.end() - 1);
    
    Neurons = Units;
    /* Initialize the layer construction */
    NNLayer *Layer;
    Layer = new NNLayer(0, inputVectorDim);
    m_Layers.push_back(Layer);
    
    int i = 0;
    for (; i < nLayers - 2; i++)
    {
        /* The parameters in constructed function is to build weight matrix */
        Layer = new NNLayer(Neurons[i], Neurons[i + 1], Layer, true);
        m_Layers.push_back(Layer);
    }
    Layer = new NNLayer(Neurons[i], outputVectorDim, Layer, false);
    m_Layers.push_back(Layer);
}

VectorXf NeuralNetwork::Calculate(VectorXf inputVector)
{
    /**
     *  Description:
     *  Feed forward in the NN
     *
     *  @param inputVector(type: VectorXf): A column vector of image Pixel
     *
     *  @return outputVector(type: VectorXf): A column vector of class
     *                                        probability
     *
     */
    
    VectorLayers::iterator lit = m_Layers.begin();
    if (lit == m_Layers.end())
    {
        std::cerr << "No Layers!" << std::endl;
    }
    
    (*lit)->NeuronsValue = inputVector;
    
    for (lit++; lit < m_Layers.end(); lit++)
    {
        (*lit)->Calculate();
    }
    
    /* Return the actual result */
    lit--;
    return (*lit)->NeuronsValue;
}

void NeuralNetwork::Backpropagate(VectorXf desireoutput, VectorXf actualOutput)
{
    /*
     *  Description:
     *  Using BP algorithm to deliver the error message and update weight
     *
     *  @param actualOutput: Generated by feed forward
     *  @param desireOutput: The standard class labels from files
     *
     */
    
    VectorLayers::iterator lit = m_Layers.end() - 1;

    /* The error message transferred in NN */
    std::vector<VectorXf> differentials;
    int iSize = (int)m_Layers.size();
    differentials.resize(iSize);   /* Every Layer stores an error message */
    
    /* The error initialization */
    VectorXf Err_dXlast((*lit)->NeuronsValue.size());
    Err_dXlast = desireoutput - actualOutput;
    for (int i = 0; i < Err_dXlast.size(); i++)
    {
        Err_dXlast[i] = Err_dXlast[i] * DSIGMOID((*lit)->NeuronsValue(i));
    }
    
    differentials[iSize - 1] = Err_dXlast;
    
    for (int i = 0; i < iSize - 1; i++)
    {
        differentials[i].resize(m_Layers[i]->NeuronsValue.size());
    }
    
    int ii = iSize - 1;
    for (lit = m_Layers.end() - 1; lit > m_Layers.begin(); lit--)
    {
        (*lit)->Backpropagate(differentials[ii], differentials[ii-1], DEFAULT_LEARNING_RATE);
        --ii;
    }
    
    differentials.clear();
}

NNLayer::NNLayer(int iLayerDim, int oLayerDim, NNLayer *pPrev, bool loadWeight): m_pPrevLayer(pPrev)
{
    /*
     *  Description:
     *  Initialize the layer and constructed weight matrix using random
     *  number if necessary
     *
     *  @param iLayerDim: The dimensionality of layer input
     *  @param oLayerDim: The dimensionality of layer output
     *  @param pPrev(NNLayer *): The previous layer
     *  @param loadWeight: If load the weight
     *
     */
    
    if (!loadWeight)
    {
        double w_inter = sqrt(6./(oLayerDim + iLayerDim + 1));
        weightMatrix = MatrixXf::Random(oLayerDim, iLayerDim) * w_inter;
        bias = VectorXf::Ones(oLayerDim);
    }
    else
    {
        weightMatrix = MatrixXf(oLayerDim, iLayerDim);
        bias = VectorXf(oLayerDim);
    }
}

void NNLayer::Calculate()
{
    /*
     *  Description:
     *  Feed forward using matrix product and add bias if needed, then
     *  mapping using sigmoid function
     *
     */
    
    NeuronsValue = weightMatrix * m_pPrevLayer->NeuronsValue + bias;
    for (int i = 0; i < NeuronsValue.size(); i++)
    {
        NeuronsValue[i] = sigmoid(NeuronsValue[i]);
    }
}

void NNLayer::Backpropagate(VectorXf &Err_dXn, VectorXf &Err_dXnm1, double etaLearningRate)
{
    /*
     *  Description:
     *  BP algorithm in every layer, using error message in current layer
     *  to update the error message in previous layer and weight matrix
     *
     *  @param Err_dXn: The error message in current layer
     *  @param Err_dXnm1: The error message in previous layer
     *  @param etaLearningRate: The step to update weight matrix
     *
     */
    
    VectorXf diff_Xnm1;
    diff_Xnm1 = weightMatrix.transpose() * Err_dXn;
    
    for (int i = 0; i < Err_dXnm1.size(); i++)
    {
        Err_dXnm1[i] = DSIGMOID((m_pPrevLayer->NeuronsValue[i])) * diff_Xnm1[i];
    }
    
    weightMatrix += etaLearningRate * Err_dXn * m_pPrevLayer->NeuronsValue.transpose();
    bias += etaLearningRate * Err_dXn;
}

void NeuralNetwork::WriteWeight(std::string str)
{
    /*
     *  Description:
     *  Save the weight matrix to reutilize instead of re-training
     *
     *  @param str: Address to save weight matrix
     *
     */
    
    std::ofstream file(str.c_str());
    
    /* Save format: One double value a line */
    VectorLayers::iterator lit = m_Layers.begin() + 1;
    for (; lit < m_Layers.end(); lit++)
    {
        for (int i = 0; i < (*lit)->weightMatrix.rows(); i++)
        {
            for (int j = 0; j < (*lit)->weightMatrix.cols(); j++)
            {
                file << (*lit)->weightMatrix(i, j) << '\n';
            }
        }
    }
    file.close();
}

void NeuralNetwork::LoadWeight(std::string str)
{
    /*
     *  Description:
     *  Load the weight matrix to reutilize instead of re-training
     *
     *  @param str: Address to load weight matrix
     *
     */
    
    std::ifstream file(str.c_str());
    
    if (!file)
    {
        std::cerr << "File doesn't exist!\n";
    }
    
    VectorLayers::iterator lit = m_Layers.begin() + 1;
    for (; lit < m_Layers.end(); lit++)
    {
        for (int i = 0; i < (*lit)->weightMatrix.rows(); i++)
        {
            for (int j = 0; j < (*lit)->weightMatrix.cols(); j++)
            {
                if (file.eof())
                {
                    std::cerr << "The weight matrix doesn't match\n";
                }
                char Weight[15];
                file.getline(Weight, 15);
                (*lit)->weightMatrix(i, j) = atof(Weight);
            }
        }
    }
    file.close();
}

void NeuralNetwork::LoadWeight(std::vector<MatrixXf> weightMatrix, std::vector<VectorXf> biasVector)
{
    /*
     *  Description:
     *  Load the weight matrix(train by other device)
     *
     *  @param weightMatrix: a vector of weight matrix
     *  @param biasVector: a vector of bias vector
     *
     */
    
    assert(nLayers == weightMatrix.size() + 2 && nLayers == biasVector.size() + 2);
    for (int i = 1; i < nLayers - 1; i++)
    {
        assert(weightMatrix[i - 1].cols() == m_Layers[i]->weightMatrix.cols());
        assert(weightMatrix[i - 1].rows() == m_Layers[i]->weightMatrix.rows());
        assert(biasVector[i - 1].size() == m_Layers[i]->bias.size());
        
        m_Layers[i]->weightMatrix = weightMatrix[i - 1];
        m_Layers[i]->bias = biasVector[i - 1];
    }
}

double sigmoid(double input)
{
    return 1./(1 + exp(-input));
}